
@article{austin_statistical_2010,
	title = {Statistical {Criteria} for {Selecting} the {Optimal} {Number} of {Untreated} {Subjects} {Matched} to {Each} {Treated} {Subject} {When} {Using} {Many}-to-{One} {Matching} on the {Propensity} {Score}},
	volume = {172},
	issn = {0002-9262},
	url = {https://doi.org/10.1093/aje/kwq224},
	doi = {10.1093/aje/kwq224},
	abstract = {Propensity-score matching is increasingly being used to estimate the effects of treatments using observational data. In many-to-one (M:1) matching on the propensity score, M untreated subjects are matched to each treated subject using the propensity score. The authors used Monte Carlo simulations to examine the effect of the choice of M on the statistical performance of matched estimators. They considered matching 1–5 untreated subjects to each treated subject using both nearest-neighbor matching and caliper matching in 96 different scenarios. Increasing the number of untreated subjects matched to each treated subject tended to increase the bias in the estimated treatment effect; conversely, increasing the number of untreated subjects matched to each treated subject decreased the sampling variability of the estimated treatment effect. Using nearest-neighbor matching, the mean squared error of the estimated treatment effect was minimized in 67.7\% of the scenarios when 1:1 matching was used. Using nearest-neighbor matching or caliper matching, the mean squared error was minimized in approximately 84\% of the scenarios when, at most, 2 untreated subjects were matched to each treated subject. The authors recommend that, in most settings, researchers match either 1 or 2 untreated subjects to each treated subject when using propensity-score matching.},
	number = {9},
	urldate = {2023-11-30},
	journal = {American Journal of Epidemiology},
	author = {Austin, Peter C.},
	month = nov,
	year = {2010},
	pages = {1092--1097},
}

@article{lingsma_covariate_2010,
	title = {Covariate adjustment increases statistical power in randomized controlled trials},
	volume = {63},
	issn = {0895-4356, 1878-5921},
	url = {https://www.jclinepi.com/article/S0895-4356(10)00188-5/fulltext},
	doi = {10.1016/j.jclinepi.2010.05.003},
	language = {English},
	number = {12},
	urldate = {2023-11-30},
	journal = {Journal of Clinical Epidemiology},
	author = {Lingsma, Hester and Roozenbeek, Bob and Steyerberg, Ewout},
	month = dec,
	year = {2010},
	pmid = {20800991},
	pages = {1391},
	annote = {Publisher: Elsevier},
}

@article{noauthor_consort_2010,
	title = {{CONSORT} 2010},
	volume = {375},
	issn = {1474-547X},
	doi = {10.1016/S0140-6736(10)60456-4},
	language = {eng},
	number = {9721},
	journal = {Lancet (London, England)},
	month = apr,
	year = {2010},
	pmid = {20338630},
	keywords = {Guideline Adherence, Guidelines as Topic, Publishing, Randomized Controlled Trials as Topic, Research Design},
	pages = {1136},
}

@misc{center_for_drug_evaluation_and_research_fda_adjusting_2023,
	title = {Adjusting for {Covariates} in {Randomized} {Clinical} {Trials} for {Drugs} and {Biological} {Products}},
	url = {https://www.fda.gov/regulatory-information/search-fda-guidance-documents/adjusting-covariates-randomized-clinical-trials-drugs-and-biological-products},
	abstract = {Adjusting for Covariates in Randomized Clinical Trials for Drugs and Biological Products},
	language = {en},
	urldate = {2023-11-30},
	author = {{Center for Drug Evaluation and Research, FDA}},
	month = may,
	year = {2023},
	annote = {Publisher: FDA},
}

@article{chatton_g-computation_2022,
	title = {G-computation and doubly robust standardisation for continuous-time data: {A} comparison with inverse probability weighting},
	volume = {31},
	issn = {1477-0334},
	shorttitle = {G-computation and doubly robust standardisation for continuous-time data},
	doi = {10.1177/09622802211047345},
	abstract = {In time-to-event settings, g-computation and doubly robust estimators are based on discrete-time data. However, many biological processes are evolving continuously over time. In this paper, we extend the g-computation and the doubly robust standardisation procedures to a continuous-time context. We compare their performance to the well-known inverse-probability-weighting estimator for the estimation of the hazard ratio and restricted mean survival times difference, using a simulation study. Under a correct model specification, all methods are unbiased, but g-computation and the doubly robust standardisation are more efficient than inverse-probability-weighting. We also analyse two real-world datasets to illustrate the practical implementation of these approaches. We have updated the R package RISCA to facilitate the use of these methods and their dissemination.},
	language = {eng},
	number = {4},
	journal = {Statistical Methods in Medical Research},
	author = {Chatton, Arthur and Borgne, Florent Le and Leyrat, Clémence and Foucher, Yohann},
	month = apr,
	year = {2022},
	pmid = {34861799},
	keywords = {Causal inference, Computer Simulation, Models, parametric g-formula, Probability, propensity score, Reference Standards, restricted mean survival time, simulation study, Statistical},
	pages = {706--718},
}

@article{colson_optimizing_2016,
	title = {Optimizing matching and analysis combinations for estimating causal effects},
	volume = {6},
	copyright = {2016 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/srep23222},
	doi = {10.1038/srep23222},
	abstract = {Matching methods are common in studies across many disciplines. However, there is limited evidence on how to optimally combine matching with subsequent analysis approaches to minimize bias and maximize efficiency for the quantity of interest. We conducted simulations to compare the performance of a wide variety of matching methods and analysis approaches in terms of bias, variance, and mean squared error (MSE). We then compared these approaches in an applied example of an employment training program. The results indicate that combining full matching with double robust analysis performed best in both the simulations and the applied example, particularly when combined with machine learning estimation methods. To reduce bias, current guidelines advise researchers to select the technique with the best post-matching covariate balance, but this work finds that such an approach does not always minimize mean squared error (MSE). These findings have important implications for future research utilizing matching. To minimize MSE, investigators should consider additional diagnostics, and use of simulations tailored to the study of interest to identify the optimal matching and analysis combination.},
	language = {en},
	number = {1},
	urldate = {2023-11-30},
	journal = {Scientific Reports},
	author = {Colson, K. Ellicott and Rudolph, Kara E. and Zimmerman, Scott C. and Goin, Dana E. and Stuart, Elizabeth A. and Laan, Mark van der and Ahern, Jennifer},
	month = mar,
	year = {2016},
	keywords = {Applied mathematics, Epidemiology, Medical research, Statistics},
	pages = {23222},
	annote = {Number: 1 Publisher: Nature Publishing Group},
}

@article{diaz_machine_2020,
	title = {Machine learning in the estimation of causal effects: targeted minimum loss-based estimation and double/debiased machine learning},
	volume = {21},
	issn = {1465-4644},
	shorttitle = {Machine learning in the estimation of causal effects},
	url = {https://doi.org/10.1093/biostatistics/kxz042},
	doi = {10.1093/biostatistics/kxz042},
	abstract = {In recent decades, the fields of statistical and machine learning have seen a revolution in the development of data-adaptive regression methods that have optimal performance under flexible, sometimes minimal, assumptions on the true regression functions. These developments have impacted all areas of applied and theoretical statistics and have allowed data analysts to avoid the biases incurred under the pervasive practice of parametric model misspecification. In this commentary, I discuss issues around the use of data-adaptive regression in estimation of causal inference parameters. To ground ideas, I focus on two estimation approaches with roots in semi-parametric estimation theory: targeted minimum loss-based estimation (TMLE; van der Laan and Rubin, 2006) and double/debiased machine learning (DML; Chernozhukov and others, 2018). This commentary is not comprehensive, the literature on these topics is rich, and there are many subtleties and developments which I do not address. These two frameworks represent only a small fraction of an increasingly large number of methods for causal inference using machine learning. To my knowledge, they are the only methods grounded in statistical semi-parametric theory that also allow unrestricted use of data-adaptive regression techniques.},
	number = {2},
	urldate = {2023-11-30},
	journal = {Biostatistics},
	author = {Díaz, Iván},
	month = apr,
	year = {2020},
	pages = {353--358},
}

@article{chatton_g-computation_2020,
	title = {G-computation, propensity score-based methods, and targeted maximum likelihood estimator for causal inference with different covariates sets: a comparative simulation study},
	volume = {10},
	issn = {2045-2322},
	shorttitle = {G-computation, propensity score-based methods, and targeted maximum likelihood estimator for causal inference with different covariates sets},
	doi = {10.1038/s41598-020-65917-x},
	abstract = {Controlling for confounding bias is crucial in causal inference. Distinct methods are currently employed to mitigate the effects of confounding bias. Each requires the introduction of a set of covariates, which remains difficult to choose, especially regarding the different methods. We conduct a simulation study to compare the relative performance results obtained by using four different sets of covariates (those causing the outcome, those causing the treatment allocation, those causing both the outcome and the treatment allocation, and all the covariates) and four methods: g-computation, inverse probability of treatment weighting, full matching and targeted maximum likelihood estimator. Our simulations are in the context of a binary treatment, a binary outcome and baseline confounders. The simulations suggest that considering all the covariates causing the outcome led to the lowest bias and variance, particularly for g-computation. The consideration of all the covariates did not decrease the bias but significantly reduced the power. We apply these methods to two real-world examples that have clinical relevance, thereby illustrating the real-world importance of using these methods. We propose an R package RISCA to encourage the use of g-computation in causal inference.},
	language = {eng},
	number = {1},
	journal = {Scientific Reports},
	author = {Chatton, Arthur and Le Borgne, Florent and Leyrat, Clémence and Gillaizeau, Florence and Rousseau, Chloé and Barbin, Laetitia and Laplaud, David and Léger, Maxime and Giraudeau, Bruno and Foucher, Yohann},
	month = jun,
	year = {2020},
	pmid = {32514028},
	pmcid = {PMC7280276},
	pages = {9219},
}

@misc{noauthor_doubly_nodate,
	title = {Doubly {Robust} {Estimation} of {Causal} {Effects} - {PMC}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3070495/},
	urldate = {2023-11-30},
}

@article{tackney_comparison_2023,
	title = {A comparison of covariate adjustment approaches under model misspecification in individually randomized trials},
	volume = {24},
	issn = {1745-6215},
	url = {https://doi.org/10.1186/s13063-022-06967-6},
	doi = {10.1186/s13063-022-06967-6},
	abstract = {Adjustment for baseline covariates in randomized trials has been shown to lead to gains in power and can protect against chance imbalances in covariates. For continuous covariates, there is a risk that the the form of the relationship between the covariate and outcome is misspecified when taking an adjusted approach. Using a simulation study focusing on individually randomized trials with small sample sizes, we explore whether a range of adjustment methods are robust to misspecification, either in the covariate–outcome relationship or through an omitted covariate–treatment interaction. Specifically, we aim to identify potential settings where G-computation, inverse probability of treatment weighting (IPTW), augmented inverse probability of treatment weighting (AIPTW) and targeted maximum likelihood estimation (TMLE) offer improvement over the commonly used analysis of covariance (ANCOVA). Our simulations show that all adjustment methods are generally robust to model misspecification if adjusting for a few covariates, sample size is 100 or larger, and there are no covariate–treatment interactions. When there is a non-linear interaction of treatment with a skewed covariate and sample size is small, all adjustment methods can suffer from bias; however, methods that allow for interactions (such as G-computation with interaction and IPTW) show improved results compared to ANCOVA. When there are a high number of covariates to adjust for, ANCOVA retains good properties while other methods suffer from under- or over-coverage. An outstanding issue for G-computation, IPTW and AIPTW in small samples is that standard errors are underestimated; they should be used with caution without the availability of small-sample corrections, development of which is needed. These findings are relevant for covariate adjustment in interim analyses of larger trials.},
	number = {1},
	urldate = {2023-12-01},
	journal = {Trials},
	author = {Tackney, Mia S. and Morris, Tim and White, Ian and Leyrat, Clemence and Diaz-Ordaz, Karla and Williamson, Elizabeth},
	month = jan,
	year = {2023},
	keywords = {AIPTW, ANCOVA, Covariate adjustment, G-computation, IPTW, Misspecification, Randomized controlled trials, TMLE},
	pages = {14},
}

@article{le_borgne_g-computation_2021,
	title = {G-computation and machine learning for estimating the causal effects of binary exposure statuses on binary outcomes},
	volume = {11},
	copyright = {2021 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-021-81110-0},
	doi = {10.1038/s41598-021-81110-0},
	abstract = {In clinical research, there is a growing interest in the use of propensity score-based methods to estimate causal effects. G-computation is an alternative because of its high statistical power. Machine learning is also increasingly used because of its possible robustness to model misspecification. In this paper, we aimed to propose an approach that combines machine learning and G-computation when both the outcome and the exposure status are binary and is able to deal with small samples. We evaluated the performances of several methods, including penalized logistic regressions, a neural network, a support vector machine, boosted classification and regression trees, and a super learner through simulations. We proposed six different scenarios characterised by various sample sizes, numbers of covariates and relationships between covariates, exposure statuses, and outcomes. We have also illustrated the application of these methods, in which they were used to estimate the efficacy of barbiturates prescribed during the first 24 h of an episode of intracranial hypertension. In the context of GC, for estimating the individual outcome probabilities in two counterfactual worlds, we reported that the super learner tended to outperform the other approaches in terms of both bias and variance, especially for small sample sizes. The support vector machine performed well, but its mean bias was slightly higher than that of the super learner. In the investigated scenarios, G-computation associated with the super learner was a performant method for drawing causal inferences, even from small sample sizes.},
	language = {en},
	number = {1},
	urldate = {2023-12-01},
	journal = {Scientific Reports},
	author = {Le Borgne, Florent and Chatton, Arthur and Léger, Maxime and Lenain, Rémi and Foucher, Yohann},
	month = jan,
	year = {2021},
	keywords = {Epidemiology, Medical research},
	pages = {1435},
	annote = {Number: 1 Publisher: Nature Publishing Group},
}

@article{funk_doubly_2011,
	title = {Doubly {Robust} {Estimation} of {Causal} {Effects}},
	volume = {173},
	issn = {0002-9262},
	url = {https://doi.org/10.1093/aje/kwq439},
	doi = {10.1093/aje/kwq439},
	abstract = {Doubly robust estimation combines a form of outcome regression with a model for the exposure (i.e., the propensity score) to estimate the causal effect of an exposure on an outcome. When used individually to estimate a causal effect, both outcome regression and propensity score methods are unbiased only if the statistical model is correctly specified. The doubly robust estimator combines these 2 approaches such that only 1 of the 2 models need be correctly specified to obtain an unbiased effect estimator. In this introduction to doubly robust estimators, the authors present a conceptual overview of doubly robust estimation, a simple worked example, results from a simulation study examining performance of estimated and bootstrapped standard errors, and a discussion of the potential advantages and limitations of this method. The supplementary material for this paper, which is posted on the Journal's Web site (http://aje.oupjournals.org/), includes a demonstration of the doubly robust property (Web Appendix 1) and a description of a SAS macro (SAS Institute, Inc., Cary, North Carolina) for doubly robust estimation, available for download at http://www.unc.edu/∼mfunk/dr/.},
	number = {7},
	urldate = {2023-12-01},
	journal = {American Journal of Epidemiology},
	author = {Funk, Michele Jonsson and Westreich, Daniel and Wiesen, Chris and Stürmer, Til and Brookhart, M. Alan and Davidian, Marie},
	month = apr,
	year = {2011},
	pages = {761--767},
}

@article{greenland_interpretation_1987,
	title = {Interpretation and choice of effect measures in epidemiologic analyses},
	volume = {125},
	issn = {0002-9262},
	doi = {10.1093/oxfordjournals.aje.a114593},
	language = {eng},
	number = {5},
	journal = {American Journal of Epidemiology},
	author = {Greenland, S.},
	month = may,
	year = {1987},
	pmid = {3551588},
	keywords = {Models, Epidemiology, Humans, Risk, Theoretical},
	pages = {761--768},
}

@article{williamson_variance_2014,
	title = {Variance reduction in randomised trials by inverse probability weighting using the propensity score},
	volume = {33},
	issn = {1097-0258},
	doi = {10.1002/sim.5991},
	abstract = {In individually randomised controlled trials, adjustment for baseline characteristics is often undertaken to increase precision of the treatment effect estimate. This is usually performed using covariate adjustment in outcome regression models. An alternative method of adjustment is to use inverse probability-of-treatment weighting (IPTW), on the basis of estimated propensity scores. We calculate the large-sample marginal variance of IPTW estimators of the mean difference for continuous outcomes, and risk difference, risk ratio or odds ratio for binary outcomes. We show that IPTW adjustment always increases the precision of the treatment effect estimate. For continuous outcomes, we demonstrate that the IPTW estimator has the same large-sample marginal variance as the standard analysis of covariance estimator. However, ignoring the estimation of the propensity score in the calculation of the variance leads to the erroneous conclusion that the IPTW treatment effect estimator has the same variance as an unadjusted estimator; thus, it is important to use a variance estimator that correctly takes into account the estimation of the propensity score. The IPTW approach has particular advantages when estimating risk differences or risk ratios. In this case, non-convergence of covariate-adjusted outcome regression models frequently occurs. Such problems can be circumvented by using the IPTW adjustment approach.},
	language = {eng},
	number = {5},
	journal = {Statistics in Medicine},
	author = {Williamson, Elizabeth J. and Forbes, Andrew and White, Ian R.},
	month = feb,
	year = {2014},
	pmid = {24114884},
	pmcid = {PMC4285308},
	keywords = {Randomized Controlled Trials as Topic, Computer Simulation, Humans, baseline adjustment, Bursitis, Odds Ratio, Physical Therapy Modalities, Propensity Score, Treatment Outcome, variance estimation},
	pages = {721--737},
}

@article{snowden_implementation_2011,
	title = {Implementation of {G}-computation on a simulated data set: demonstration of a causal inference technique},
	volume = {173},
	issn = {1476-6256},
	shorttitle = {Implementation of {G}-computation on a simulated data set},
	doi = {10.1093/aje/kwq472},
	abstract = {The growing body of work in the epidemiology literature focused on G-computation includes theoretical explanations of the method but very few simulations or examples of application. The small number of G-computation analyses in the epidemiology literature relative to other causal inference approaches may be partially due to a lack of didactic explanations of the method targeted toward an epidemiology audience. The authors provide a step-by-step demonstration of G-computation that is intended to familiarize the reader with this procedure. The authors simulate a data set and then demonstrate both G-computation and traditional regression to draw connections and illustrate contrasts between their implementation and interpretation relative to the truth of the simulation protocol. A marginal structural model is used for effect estimation in the G-computation example. The authors conclude by answering a series of questions to emphasize the key characteristics of causal inference techniques and the G-computation procedure in particular.},
	language = {eng},
	number = {7},
	journal = {American Journal of Epidemiology},
	author = {Snowden, Jonathan M. and Rose, Sherri and Mortimer, Kathleen M.},
	month = apr,
	year = {2011},
	pmid = {21415029},
	pmcid = {PMC3105284},
	keywords = {Computer Simulation, Models, Statistical, Humans, Causality, Confounding Factors, Epidemiologic, Epidemiologic Research Design, Regression Analysis},
	pages = {731--738},
}

@article{lendle_targeted_2013,
	series = {Methods for {Comparative} {Effectiveness} {Research}/{Patient}-{Centered} {Outcomes} {Research}: {From} {Efficacy} to {Effectiveness}},
	title = {Targeted maximum likelihood estimation in safety analysis},
	volume = {66},
	issn = {0895-4356},
	url = {https://www.sciencedirect.com/science/article/pii/S0895435613002011},
	doi = {10.1016/j.jclinepi.2013.02.017},
	abstract = {Objectives To compare the performance of a targeted maximum likelihood estimator (TMLE) and a collaborative TMLE (CTMLE) to other estimators in a drug safety analysis, including a regression-based estimator, propensity score (PS)–based estimators, and an alternate doubly robust (DR) estimator in a real example and simulations. Study Design and Setting The real data set is a subset of observational data from Kaiser Permanente Northern California formatted for use in active drug safety surveillance. Both the real and simulated data sets include potential confounders, a treatment variable indicating use of one of two antidiabetic treatments and an outcome variable indicating occurrence of an acute myocardial infarction (AMI). Results In the real data example, there is no difference in AMI rates between treatments. In simulations, the double robustness property is demonstrated: DR estimators are consistent if either the initial outcome regression or PS estimator is consistent, whereas other estimators are inconsistent if the initial estimator is not consistent. In simulations with near-positivity violations, CTMLE performs well relative to other estimators by adaptively estimating the PS. Conclusion Each of the DR estimators was consistent, and TMLE and CTMLE had the smallest mean squared error in simulations.},
	number = {8, Supplement},
	urldate = {2023-12-06},
	journal = {Journal of Clinical Epidemiology},
	author = {Lendle, Samuel D. and Fireman, Bruce and {van der Laan}, Mark J.},
	month = aug,
	year = {2013},
	keywords = {Causal inference, Collaborative targeted maximum likelihood estimation, Doubly robust, Safety analysis, Super learning, Targeted maximum likelihood estimation},
	pages = {S91--S98},
}

@article{zeng_propensity_2021,
	title = {Propensity score weighting for covariate adjustment in randomized clinical trials},
	volume = {40},
	copyright = {© 2020 John Wiley \& Sons Ltd},
	issn = {1097-0258},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.8805},
	doi = {10.1002/sim.8805},
	abstract = {Chance imbalance in baseline characteristics is common in randomized clinical trials. Regression adjustment such as the analysis of covariance (ANCOVA) is often used to account for imbalance and increase precision of the treatment effect estimate. An objective alternative is through inverse probability weighting (IPW) of the propensity scores. Although IPW and ANCOVA are asymptotically equivalent, the former may demonstrate inferior performance in finite samples. In this article, we point out that IPW is a special case of the general class of balancing weights, and advocate to use overlap weighting (OW) for covariate adjustment. The OW method has a unique advantage of completely removing chance imbalance when the propensity score is estimated by logistic regression. We show that the OW estimator attains the same semiparametric variance lower bound as the most efficient ANCOVA estimator and the IPW estimator for a continuous outcome, and derive closed-form variance estimators for OW when estimating additive and ratio estimands. Through extensive simulations, we demonstrate OW consistently outperforms IPW in finite samples and improves the efficiency over ANCOVA and augmented IPW when the degree of treatment effect heterogeneity is moderate or when the outcome model is incorrectly specified. We apply the proposed OW estimator to the Best Apnea Interventions for Research (BestAIR) randomized trial to evaluate the effect of continuous positive airway pressure on patient health outcomes. All the discussed propensity score weighting methods are implemented in the R package PSweight.},
	language = {en},
	number = {4},
	urldate = {2023-12-06},
	journal = {Statistics in Medicine},
	author = {Zeng, Shuxi and Li, Fan and Wang, Rui and Li, Fan},
	year = {2021},
	keywords = {analysis of covariance, covariate balance, inverse probability weighting, overlap weighting, randomized controlled trials, variance reduction},
	pages = {842--858},
	annote = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.8805},
}

@article{daniel_making_2021,
	title = {Making apples from oranges: {Comparing} noncollapsible effect estimators and their standard errors after adjustment for different covariate sets},
	volume = {63},
	copyright = {© 2020 The Authors. Biometrical Journal published by Wiley-VCH GmbH.},
	issn = {1521-4036},
	shorttitle = {Making apples from oranges},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/bimj.201900297},
	doi = {10.1002/bimj.201900297},
	abstract = {We revisit the well-known but often misunderstood issue of (non)collapsibility of effect measures in regression models for binary and time-to-event outcomes. We describe an existing simple but largely ignored procedure for marginalizing estimates of conditional odds ratios and propose a similar procedure for marginalizing estimates of conditional hazard ratios (allowing for right censoring), demonstrating its performance in simulation studies and in a reanalysis of data from a small randomized trial in primary biliary cirrhosis patients. In addition, we aim to provide an educational summary of issues surrounding (non)collapsibility from a causal inference perspective and to promote the idea that the words conditional and adjusted (likewise marginal and unadjusted) should not be used interchangeably.},
	language = {en},
	number = {3},
	urldate = {2023-12-06},
	journal = {Biometrical Journal},
	author = {Daniel, Rhian and Zhang, Jingjing and Farewell, Daniel},
	year = {2021},
	keywords = {covariate adjustment, Cox proportional hazards regression, logistic regression, noncollapsibility},
	pages = {528--557},
	annote = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/bimj.201900297},
}

@article{noel_daclizumab_2009,
	title = {Daclizumab versus antithymocyte globulin in high-immunological-risk renal transplant recipients},
	volume = {20},
	issn = {1533-3450},
	doi = {10.1681/ASN.2008101037},
	abstract = {Nondepleting anti-CD25 monoclonal antibodies (daclizumab) and depleting polyclonal antithymocyte globulin (Thymoglobulin) both prevent acute rejection, but these therapies have not been directly compared in a high-risk, HLA-sensitized renal transplant population. We randomly assigned 227 patients, who were about to receive a kidney graft from a deceased donor, to either Thymoglobulin or daclizumab if they met one of the following risk factors: current panel reactive antibodies (PRA) {\textgreater}30\%; peak PRA {\textgreater}50\%; loss of a first kidney graft from rejection within 2 yr of transplantation; or two or three previous grafts. Maintenance immunosuppression comprised tacrolimus, mycophenolate mofetil, and steroids. Compared with the daclizumab group, patients treated with Thymoglobulin had a lower incidence of both biopsy-proven acute rejection (15.0\% versus 27.2\%; P = 0.016) and steroid-resistant rejection (2.7\% versus 14.9\%; P = 0.002) at one year. One-year graft and patient survival rates were similar between the two groups. In a comparison of rejectors and nonrejectors, overall graft survival was significantly higher in the rejection-free group (87.2\% versus 75.0\%; P = 0.037). In conclusion, among high-immunological-risk renal transplant recipients, Thymoglobulin is superior to daclizumab for the prevention of biopsy-proven acute rejection, but there is no significant benefit to one-year graft or patient survival.},
	language = {eng},
	number = {6},
	journal = {Journal of the American Society of Nephrology: JASN},
	author = {Noël, Christian and Abramowicz, Daniel and Durand, Dominique and Mourad, Georges and Lang, Philippe and Kessler, Michèle and Charpentier, Bernard and Touchard, Guy and Berthoux, François and Merville, Pierre and Ouali, Nacera and Squifflet, Jean-Paul and Bayle, François and Wissing, Karl Martin and Hazzan, Marc},
	month = jun,
	year = {2009},
	pmid = {19470677},
	pmcid = {PMC2689909},
	keywords = {Humans, Treatment Outcome, Adult, Antibodies, Monoclonal, Antibodies, Monoclonal, Humanized, Antilymphocyte Serum, Biopsy, Daclizumab, Female, Graft Rejection, Immunoglobulin G, Immunosuppressive Agents, Kidney, Kidney Transplantation, Male, Middle Aged, Prospective Studies, Risk Factors},
	pages = {1385--1392},
}

@article{thille_non-invasive_2021,
	title = {Non-invasive ventilation versus high-flow nasal oxygen for postextubation respiratory failure in {ICU}: a post-hoc analysis of a randomized clinical trial},
	volume = {25},
	issn = {1466-609X},
	shorttitle = {Non-invasive ventilation versus high-flow nasal oxygen for postextubation respiratory failure in {ICU}},
	doi = {10.1186/s13054-021-03621-6},
	abstract = {BACKGROUND: In intensive care units (ICUs), patients experiencing post-extubation respiratory failure have poor outcomes. The use of noninvasive ventilation (NIV) to treat post-extubation respiratory failure may increase the risk of death. This study aims at comparing mortality between patients treated with NIV alternating with high-flow nasal oxygen or high-flow nasal oxygen alone.
METHODS: Post-hoc analysis of a multicenter, randomized, controlled trial focusing on patients who experienced post-extubation respiratory failure within the 7 days following extubation. Patients were classified in the NIV group or the high-flow nasal oxygen group according to oxygenation strategy used after the onset of post-extubation respiratory failure. Patients reintubated within the first hour after extubation and those promptly reintubated without prior treatment were excluded. The primary outcome was mortality at day 28 after the onset of post-extubation respiratory failure.
RESULTS: Among 651 extubated patients, 158 (25\%) experienced respiratory failure and 146 were included in the analysis. Mortality at day 28 was 18\% (15/84) using NIV alternating with high-flow nasal oxygen and 29\% (18/62) with high flow nasal oxygen alone (difference, - 11\% [95\% CI, - 25 to 2]; p = 0.12). Among the 46 patients with hypercapnia at the onset of respiratory failure, mortality at day 28 was 3\% (1/33) with NIV and 31\% (4/13) with high-flow nasal oxygen alone (difference, - 28\% [95\% CI, - 54 to - 6]; p = 0.006). The proportion of patients reintubated 48 h after the onset of post-extubation respiratory failure was 44\% (37/84) with NIV and 52\% (32/62) with high-flow nasal oxygen alone (p = 0.21).
CONCLUSIONS: In patients with post-extubation respiratory failure, NIV alternating with high-flow nasal oxygen might not increase the risk of death. Trial registration number The trial was registered at http://www.clinicaltrials.gov with the registration number NCT03121482 the 20th April 2017.},
	language = {eng},
	number = {1},
	journal = {Critical Care (London, England)},
	author = {Thille, Arnaud W. and Monseau, Grégoire and Coudroy, Rémi and Nay, Mai-Anh and Gacouin, Arnaud and Decavèle, Maxens and Sonneville, Romain and Beloncle, François and Girault, Christophe and Dangers, Laurence and Lautrette, Alexandre and Levrat, Quentin and Rouzé, Anahita and Vivier, Emmanuel and Lascarrou, Jean-Baptiste and Ricard, Jean-Damien and Razazi, Keyvan and Barberet, Guillaume and Lebert, Christine and Ehrmann, Stephan and Massri, Alexandre and Bourenne, Jeremy and Pradel, Gael and Bailly, Pierre and Terzi, Nicolas and Dellamonica, Jean and Lacave, Guillaume and Robert, René and Ragot, Stéphanie and Frat, Jean-Pierre and {HIGH-WEAN Study Group and the REVA research network}},
	month = jun,
	year = {2021},
	pmid = {34183053},
	pmcid = {PMC8236736},
	keywords = {Humans, Female, Male, Middle Aged, Acute respiratory failure, Aged, Aged, 80 and over, Airway extubation, Airway Extubation, High-flow nasal oxygen, Intensive Care Units, Kaplan-Meier Estimate, Length of Stay, Noninvasive ventilation, Noninvasive Ventilation, Oxygen Inhalation Therapy, Respiratory Insufficiency, Ventilator weaning},
	pages = {221},
	file = {Full Text:C\:\\Users\\Joe\\Zotero\\storage\\QI943828\\Thille et al. - 2021 - Non-invasive ventilation versus high-flow nasal ox.pdf:application/pdf},
}

@article{heymans_handling_2022,
	title = {Handling missing data in clinical research},
	volume = {151},
	issn = {1878-5921},
	doi = {10.1016/j.jclinepi.2022.08.016},
	abstract = {Because missing data are present in almost every study, it is important to handle missing data properly. First of all, the missing data mechanism should be considered. Missing data can be either completely at random (MCAR), at random (MAR), or not at random (MNAR). When missing data are MCAR, a complete case analysis can be valid. Also when missing data are MAR, in some situations a complete case analysis leads to valid results. However, in most situations, missing data imputation should be used. Regarding imputation methods, it is highly advised to use multiple imputations because multiple imputations lead to valid estimates including the uncertainty about the imputed values. When missing data are MNAR, also multiple imputations do not lead to valid results. A complication hereby is that it not possible to distinguish whether missing data are MAR or MNAR. Finally, it should be realized that preventing to have missing data is always better than the treatment of missing data.},
	language = {eng},
	journal = {Journal of Clinical Epidemiology},
	author = {Heymans, Martijn W. and Twisk, Jos W. R.},
	month = nov,
	year = {2022},
	pmid = {36150546},
	keywords = {Humans, Uncertainty},
	pages = {185--188},
	file = {Full Text:C\:\\Users\\Joe\\Zotero\\storage\\7Z2CH6P5\\Heymans and Twisk - 2022 - Handling missing data in clinical research.pdf:application/pdf},
}

@article{tackney_comparison_2023-1,
	title = {A comparison of covariate adjustment approaches under model misspecification in individually randomized trials},
	volume = {24},
	issn = {1745-6215},
	doi = {10.1186/s13063-022-06967-6},
	abstract = {Adjustment for baseline covariates in randomized trials has been shown to lead to gains in power and can protect against chance imbalances in covariates. For continuous covariates, there is a risk that the the form of the relationship between the covariate and outcome is misspecified when taking an adjusted approach. Using a simulation study focusing on individually randomized trials with small sample sizes, we explore whether a range of adjustment methods are robust to misspecification, either in the covariate-outcome relationship or through an omitted covariate-treatment interaction. Specifically, we aim to identify potential settings where G-computation, inverse probability of treatment weighting (IPTW), augmented inverse probability of treatment weighting (AIPTW) and targeted maximum likelihood estimation (TMLE) offer improvement over the commonly used analysis of covariance (ANCOVA). Our simulations show that all adjustment methods are generally robust to model misspecification if adjusting for a few covariates, sample size is 100 or larger, and there are no covariate-treatment interactions. When there is a non-linear interaction of treatment with a skewed covariate and sample size is small, all adjustment methods can suffer from bias; however, methods that allow for interactions (such as G-computation with interaction and IPTW) show improved results compared to ANCOVA. When there are a high number of covariates to adjust for, ANCOVA retains good properties while other methods suffer from under- or over-coverage. An outstanding issue for G-computation, IPTW and AIPTW in small samples is that standard errors are underestimated; they should be used with caution without the availability of small-sample corrections, development of which is needed. These findings are relevant for covariate adjustment in interim analyses of larger trials.},
	language = {eng},
	number = {1},
	journal = {Trials},
	author = {Tackney, Mia S. and Morris, Tim and White, Ian and Leyrat, Clemence and Diaz-Ordaz, Karla and Williamson, Elizabeth},
	month = jan,
	year = {2023},
	pmid = {36609282},
	pmcid = {PMC9817411},
	keywords = {Randomized Controlled Trials as Topic, Computer Simulation, Probability, AIPTW, ANCOVA, Covariate adjustment, G-computation, IPTW, Misspecification, Randomized controlled trials, TMLE, Humans, Sample Size},
	pages = {14},
	file = {Full Text:C\:\\Users\\Joe\\Zotero\\storage\\IGJQY2PD\\Tackney et al. - 2023 - A comparison of covariate adjustment approaches un.pdf:application/pdf},
}

@article{debout_each_2015,
	title = {Each additional hour of cold ischemia time significantly increases the risk of graft failure and mortality following renal transplantation},
	volume = {87},
	issn = {1523-1755},
	doi = {10.1038/ki.2014.304},
	abstract = {Although cold ischemia time has been widely studied in renal transplantation area, there is no consensus on its precise relationship with the transplantation outcomes. To study this, we sampled data from 3839 adult recipients of a first heart-beating deceased donor kidney transplanted between 2000 and 2011 within the French observational multicentric prospective DIVAT cohort. A Cox model was used to assess the relationship between cold ischemia time and death-censored graft survival or patient survival by using piecewise log-linear function. There was a significant proportional increase in the risk of graft failure for each additional hour of cold ischemia time (hazard ratio, 1.013). As an example, a patient who received a kidney with a cold ischemia time of 30 h presented a risk of graft failure near 40\% higher than a patient with a cold ischemia time of 6 h. Moreover, we found that the risk of death also proportionally increased for each additional hour of cold ischemia time (hazard ratio, 1.018). Thus, every additional hour of cold ischemia time must be taken into account in order to increase graft and patient survival. These findings are of practical clinical interest, as cold ischemia time is among one of the main modifiable pre-transplantation risk factors that can be minimized by improved management of the peri-transplantation period.},
	language = {eng},
	number = {2},
	journal = {Kidney International},
	author = {Debout, Agnes and Foucher, Yohann and Trébern-Launay, Katy and Legendre, Christophe and Kreis, Henri and Mourad, Georges and Garrigue, Valérie and Morelon, Emmanuel and Buron, Fanny and Rostaing, Lionel and Kamar, Nassim and Kessler, Michèle and Ladrière, Marc and Poignas, Alexandra and Blidi, Amina and Soulillou, Jean-Paul and Giral, Magali and Dantan, Etienne},
	month = feb,
	year = {2015},
	pmid = {25229341},
	keywords = {Humans, Adult, Female, Kidney Transplantation, Male, Middle Aged, Prospective Studies, Risk Factors, Aged, Cohort Studies, Cold Ischemia, France, Graft Survival, Proportional Hazards Models, Survival Analysis, Time Factors, Treatment Failure},
	pages = {343--349},
	file = {Submitted Version:C\:\\Users\\Joe\\Zotero\\storage\\LS3PBWER\\Debout et al. - 2015 - Each additional hour of cold ischemia time signifi.pdf:application/pdf},
}

@article{le_borgne_g-computation_2021-1,
	title = {G-computation and machine learning for estimating the causal effects of binary exposure statuses on binary outcomes},
	volume = {11},
	copyright = {2021 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-021-81110-0},
	doi = {10.1038/s41598-021-81110-0},
	abstract = {In clinical research, there is a growing interest in the use of propensity score-based methods to estimate causal effects. G-computation is an alternative because of its high statistical power. Machine learning is also increasingly used because of its possible robustness to model misspecification. In this paper, we aimed to propose an approach that combines machine learning and G-computation when both the outcome and the exposure status are binary and is able to deal with small samples. We evaluated the performances of several methods, including penalized logistic regressions, a neural network, a support vector machine, boosted classification and regression trees, and a super learner through simulations. We proposed six different scenarios characterised by various sample sizes, numbers of covariates and relationships between covariates, exposure statuses, and outcomes. We have also illustrated the application of these methods, in which they were used to estimate the efficacy of barbiturates prescribed during the first 24 h of an episode of intracranial hypertension. In the context of GC, for estimating the individual outcome probabilities in two counterfactual worlds, we reported that the super learner tended to outperform the other approaches in terms of both bias and variance, especially for small sample sizes. The support vector machine performed well, but its mean bias was slightly higher than that of the super learner. In the investigated scenarios, G-computation associated with the super learner was a performant method for drawing causal inferences, even from small sample sizes.},
	language = {en},
	number = {1},
	urldate = {2024-06-26},
	journal = {Scientific Reports},
	author = {Le Borgne, Florent and Chatton, Arthur and Léger, Maxime and Lenain, Rémi and Foucher, Yohann},
	month = jan,
	year = {2021},
	note = {Publisher: Nature Publishing Group},
	keywords = {Epidemiology, Medical research},
	pages = {1435},
	file = {Full Text PDF:C\:\\Users\\Joe\\Zotero\\storage\\DVJATTGW\\Le Borgne et al. - 2021 - G-computation and machine learning for estimating .pdf:application/pdf},
}

@article{kang_demystifying_2007,
	title = {Demystifying {Double} {Robustness}: {A} {Comparison} of {Alternative} {Strategies} for {Estimating} a {Population} {Mean} from {Incomplete} {Data}},
	volume = {22},
	issn = {0883-4237, 2168-8745},
	shorttitle = {Demystifying {Double} {Robustness}},
	url = {https://projecteuclid.org/journals/statistical-science/volume-22/issue-4/Demystifying-Double-Robustness--A-Comparison-of-Alternative-Strategies-for/10.1214/07-STS227.full},
	doi = {10.1214/07-STS227},
	abstract = {When outcomes are missing for reasons beyond an investigator’s control, there are two different ways to adjust a parameter estimate for covariates that may be related both to the outcome and to missingness. One approach is to model the relationships between the covariates and the outcome and use those relationships to predict the missing values. Another is to model the probabilities of missingness given the covariates and incorporate them into a weighted or stratified estimate. Doubly robust (DR) procedures apply both types of model simultaneously and produce a consistent estimate of the parameter if either of the two models has been correctly specified. In this article, we show that DR estimates can be constructed in many ways. We compare the performance of various DR and non-DR estimates of a population mean in a simulated example where both models are incorrect but neither is grossly misspecified. Methods that use inverse-probabilities as weights, whether they are DR or not, are sensitive to misspecification of the propensity model when some estimated propensities are small. Many DR methods perform better than simple inverse-probability weighting. None of the DR methods we tried, however, improved upon the performance of simple regression-based prediction of the missing values. This study does not represent every missing-data problem that will arise in practice. But it does demonstrate that, in at least some settings, two wrong models are not better than one.},
	number = {4},
	urldate = {2024-06-26},
	journal = {Statistical Science},
	author = {Kang, Joseph D. Y. and Schafer, Joseph L.},
	month = nov,
	year = {2007},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {Causal inference, propensity score, missing data, model-assisted survey estimation, weighted estimating equations},
	pages = {523--539},
	file = {Full Text PDF:C\:\\Users\\Joe\\Zotero\\storage\\ZEZ6QM77\\Kang and Schafer - 2007 - Demystifying Double Robustness A Comparison of Al.pdf:application/pdf},
}

@article{frat_high-flow_2015,
	title = {High-flow oxygen through nasal cannula in acute hypoxemic respiratory failure},
	volume = {372},
	issn = {1533-4406},
	doi = {10.1056/NEJMoa1503326},
	abstract = {BACKGROUND: Whether noninvasive ventilation should be administered in patients with acute hypoxemic respiratory failure is debated. Therapy with high-flow oxygen through a nasal cannula may offer an alternative in patients with hypoxemia.
METHODS: We performed a multicenter, open-label trial in which we randomly assigned patients without hypercapnia who had acute hypoxemic respiratory failure and a ratio of the partial pressure of arterial oxygen to the fraction of inspired oxygen of 300 mm Hg or less to high-flow oxygen therapy, standard oxygen therapy delivered through a face mask, or noninvasive positive-pressure ventilation. The primary outcome was the proportion of patients intubated at day 28; secondary outcomes included all-cause mortality in the intensive care unit and at 90 days and the number of ventilator-free days at day 28.
RESULTS: A total of 310 patients were included in the analyses. The intubation rate (primary outcome) was 38\% (40 of 106 patients) in the high-flow-oxygen group, 47\% (44 of 94) in the standard group, and 50\% (55 of 110) in the noninvasive-ventilation group (P=0.18 for all comparisons). The number of ventilator-free days at day 28 was significantly higher in the high-flow-oxygen group (24±8 days, vs. 22±10 in the standard-oxygen group and 19±12 in the noninvasive-ventilation group; P=0.02 for all comparisons). The hazard ratio for death at 90 days was 2.01 (95\% confidence interval [CI], 1.01 to 3.99) with standard oxygen versus high-flow oxygen (P=0.046) and 2.50 (95\% CI, 1.31 to 4.78) with noninvasive ventilation versus high-flow oxygen (P=0.006).
CONCLUSIONS: In patients with nonhypercapnic acute hypoxemic respiratory failure, treatment with high-flow oxygen, standard oxygen, or noninvasive ventilation did not result in significantly different intubation rates. There was a significant difference in favor of high-flow oxygen in 90-day mortality. (Funded by the Programme Hospitalier de Recherche Clinique Interrégional 2010 of the French Ministry of Health; FLORALI ClinicalTrials.gov number, NCT01320384.).},
	language = {eng},
	number = {23},
	journal = {The New England Journal of Medicine},
	author = {Frat, Jean-Pierre and Thille, Arnaud W. and Mercat, Alain and Girault, Christophe and Ragot, Stéphanie and Perbet, Sébastien and Prat, Gwénael and Boulain, Thierry and Morawiec, Elise and Cottereau, Alice and Devaquet, Jérôme and Nseir, Saad and Razazi, Keyvan and Mira, Jean-Paul and Argaud, Laurent and Chakarian, Jean-Charles and Ricard, Jean-Damien and Wittebole, Xavier and Chevalier, Stéphanie and Herbland, Alexandre and Fartoukh, Muriel and Constantin, Jean-Michel and Tonnelier, Jean-Marie and Pierrot, Marc and Mathonnet, Armelle and Béduneau, Gaëtan and Delétage-Métreau, Céline and Richard, Jean-Christophe M. and Brochard, Laurent and Robert, René and {FLORALI Study Group} and {REVA Network}},
	month = jun,
	year = {2015},
	pmid = {25981908},
	keywords = {Humans, Adult, Female, Male, Middle Aged, Aged, Kaplan-Meier Estimate, Oxygen Inhalation Therapy, Respiratory Insufficiency, Acute Disease, Hypoxia, Intubation, Intratracheal, Oxygen, Positive-Pressure Respiration},
	pages = {2185--2196},
	file = {Full Text:C\:\\Users\\Joe\\Zotero\\storage\\AG6FCNWJ\\Frat et al. - 2015 - High-flow oxygen through nasal cannula in acute hy.pdf:application/pdf},
}

@article{leger_causal_2022,
	title = {Causal inference in case of near-violation of positivity: comparison of methods},
	volume = {64},
	issn = {1521-4036},
	shorttitle = {Causal inference in case of near-violation of positivity},
	doi = {10.1002/bimj.202000323},
	abstract = {In causal studies, the near-violation of the positivity may occur by chance, because of sample-to-sample fluctuation despite the theoretical veracity of the positivity assumption in the population. It may mostly happen when the exposure prevalence is low or when the sample size is small. We aimed to compare the robustness of g-computation (GC), inverse probability weighting (IPW), truncated IPW, targeted maximum likelihood estimation (TMLE), and truncated TMLE in this situation, using simulations and one real application. We also tested different extrapolation situations for the sub-group with a positivity violation. The results illustrated that the near-violation of the positivity impacted all methods. We demonstrated the robustness of GC and TMLE-based methods. Truncation helped in limiting the bias in near-violation situations, but at the cost of bias in normal conditions. The application illustrated the variability of the results between the methods and the importance of choosing the most appropriate one. In conclusion, compared to propensity score-based methods, methods based on outcome regression should be preferred when suspecting near-violation of the positivity assumption.},
	language = {eng},
	number = {8},
	journal = {Biometrical Journal. Biometrische Zeitschrift},
	author = {Léger, Maxime and Chatton, Arthur and Le Borgne, Florent and Pirracchio, Romain and Lasocki, Sigismond and Foucher, Yohann},
	month = dec,
	year = {2022},
	pmid = {34993990},
	keywords = {Computer Simulation, propensity score, Propensity Score, Causality, Bias, causal inference, doubly robust estimators, g-computation, Likelihood Functions, Models, Statistical, positivity, real-world evidence, simulations},
	pages = {1389--1403},
}

@article{steingrimsson_improving_2017,
	title = {Improving precision by adjusting for prognostic baseline variables in randomized trials with binary outcomes, without regression model assumptions},
	volume = {54},
	issn = {1559-2030},
	doi = {10.1016/j.cct.2016.12.026},
	abstract = {In randomized clinical trials with baseline variables that are prognostic for the primary outcome, there is potential to improve precision and reduce sample size by appropriately adjusting for these variables. A major challenge is that there are multiple statistical methods to adjust for baseline variables, but little guidance on which is best to use in a given context. The choice of method can have important consequences. For example, one commonly used method leads to uninterpretable estimates if there is any treatment effect heterogeneity, which would jeopardize the validity of trial conclusions. We give practical guidance on how to avoid this problem, while retaining the advantages of covariate adjustment. This can be achieved by using simple (but less well-known) standardization methods from the recent statistics literature. We discuss these methods and give software in R and Stata implementing them. A data example from a recent stroke trial is used to illustrate these methods.},
	language = {eng},
	journal = {Contemporary Clinical Trials},
	author = {Steingrimsson, Jon Arni and Hanley, Daniel F. and Rosenblum, Michael},
	month = mar,
	year = {2017},
	pmid = {28064029},
	keywords = {Randomized Controlled Trials as Topic, Covariate adjustment, Humans, Sample Size, Cerebral Intraventricular Hemorrhage, Fibrinolytic Agents, Logistic Models, Post-stratification, Prognosis, Statistics as Topic, Stroke, Tissue Plasminogen Activator},
	pages = {18--24},
}

@misc{wang_model-robust_2020,
	title = {Model-{Robust} {Inference} for {Clinical} {Trials} that {Improve} {Precision} by {Stratified} {Randomization} and {Covariate} {Adjustment}},
	url = {http://arxiv.org/abs/1910.13954},
	doi = {10.48550/arXiv.1910.13954},
	abstract = {Two commonly used methods for improving precision and power in clinical trials are stratified randomization and covariate adjustment. However, many trials do not fully capitalize on the combined precision gains from these two methods, which can lead to wasted resources in terms of sample size and trial duration. We derive consistency and asymptotic normality of model-robust estimators that combine these two methods, and show that these estimators can lead to substantial gains in precision and power. Our theorems cover a class of estimators that handle continuous, binary, and time-to-event outcomes; missing outcomes under the missing at random assumption are handled as well. For each estimator, we give a formula for a consistent variance estimator that is model-robust and that fully captures variance reductions from stratified randomization and covariate adjustment. Also, we give the first proof (to the best of our knowledge) of consistency and asymptotic normality of the Kaplan-Meier estimator under stratified randomization, and we derive its asymptotic variance. The above results also hold for the biased-coin covariate-adaptive design. We demonstrate our results using three completed, phase 3, randomized trial data sets of treatments for substance use disorder, where the variance reduction due to stratified randomization and covariate adjustment ranges from 1\% to 36\%.},
	urldate = {2024-06-26},
	publisher = {arXiv},
	author = {Wang, Bingkai and Susukida, Ryoko and Mojtabai, Ramin and Amin-Esmaeili, Masoumeh and Rosenblum, Michael},
	month = sep,
	year = {2020},
	note = {arXiv:1910.13954 [stat]},
	keywords = {Statistics - Methodology},
	file = {arXiv Fulltext PDF:C\:\\Users\\Joe\\Zotero\\storage\\ADRGMFX8\\Wang et al. - 2020 - Model-Robust Inference for Clinical Trials that Im.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Joe\\Zotero\\storage\\C8QAEX52\\1910.html:text/html},
}

@article{anthon_overall_2019,
	title = {Overall bias and sample sizes were unchanged in {ICU} trials over time: a meta-epidemiological study},
	volume = {113},
	issn = {1878-5921},
	shorttitle = {Overall bias and sample sizes were unchanged in {ICU} trials over time},
	doi = {10.1016/j.jclinepi.2019.05.021},
	abstract = {OBJECTIVE: To assess time trends in risk of bias (RoB) and sample sizes in randomized clinical trials (RCTs) of adult intensive care unit (ICU) patients.
STUDY DESIGN AND SETTING: A meta-epidemiological study of RCTs from Cochrane systematic reviews assessing interventions in adult ICU patients. Using run charts, we assessed time trends in the annual proportion of RCTs with overall low RoB, the annual median sample sizes, and the annual proportion of RCTs with low, unclear, and high RoB in individual bias domains.
RESULTS: We included 604 RCTs published between 1977 and 2018 from 53 Cochrane systematic reviews. Only 6.8\% of the RCTs had overall low RoB. We observed only random variation in the annual proportions of RCTs with overall low RoB, in the annual median sample sizes and in most individual bias domains. For "allocation concealment," we observed an increase in the proportion of low RoB RCTs and a decrease in the unclear RoB RCTs.
CONCLUSIONS: Few RCTs in adult ICU patients had overall low RoB. We found no evidence of an increase in RCTs with overall low RoB or in the median sample sizes over time. The only individual RoB domain with better ratings over time was "allocation concealment."},
	language = {eng},
	journal = {Journal of Clinical Epidemiology},
	author = {Anthon, Carl Thomas and Granholm, Anders and Perner, Anders and Laake, Jon Henrik and Møller, Morten Hylander},
	month = sep,
	year = {2019},
	pmid = {31150836},
	keywords = {Randomized Controlled Trials as Topic, Humans, Intensive Care Units, Sample Size, Bias, Epidemiologic Studies, Forecasting, Intensive care, Intensive care unit, Randomized clinical trials, Research methodology, Research Report, Risk of bias, Sample size},
	pages = {189--199},
}

@article{robinson_characteristics_2021,
	title = {Characteristics of {Randomized} {Clinical} {Trials} in {Surgery} {From} 2008 to 2020: {A} {Systematic} {Review}},
	volume = {4},
	issn = {2574-3805},
	shorttitle = {Characteristics of {Randomized} {Clinical} {Trials} in {Surgery} {From} 2008 to 2020},
	doi = {10.1001/jamanetworkopen.2021.14494},
	abstract = {IMPORTANCE: Randomized clinical trials (RCTs) provide the highest level of evidence to evaluate 2 or more surgical interventions. Surgical RCTs, however, face unique challenges in design and implementation.
OBJECTIVE: To evaluate the design, conduct, and reporting of contemporary surgical RCTs.
EVIDENCE REVIEW: A literature search performed in the 2 journals with the highest impact factor in general medicine as well as 6 key surgical specialties was conducted to identify RCTs published between 2008 and 2020. All RCTs describing a surgical intervention in both experimental and control arms were included. The quality of included data was assessed by establishing an a priori protocol containing all the details to extract. Trial characteristics, fragility index, risk of bias (Cochrane Risk of Bias 2 Tool), pragmatism (Pragmatic Explanatory Continuum Indicator Summary 2 [PRECIS-2]), and reporting bias were assessed.
FINDINGS: A total of 388 trials were identified. Of them, 242 (62.4\%) were registered; discrepancies with the published protocol were identified in 81 (33.5\%). Most trials used superiority design (329 [84.8\%]), and intention-to-treat as primary analysis (221 [56.9\%]) and were designed to detect a large treatment effect (50.0\%; interquartile range [IQR], 24.7\%-63.3\%). Only 123 trials (31.7\%) used major clinical events as the primary outcome. Most trials (303 [78.1\%]) did not control for surgeon experience; only 17 trials (4.4\%) assessed the quality of the intervention. The median sample size was 122 patients (IQR, 70-245 patients). The median follow-up was 24 months (IQR, 12.0-32.0 months). Most trials (211 [54.4\%]) had some concern of bias and 91 (23.5\%) had high risk of bias. The mean (SD) PRECIS-2 score was 3.52 (0.65) and increased significantly over the study period. Most trials (212 [54.6\%]) reported a neutral result; reporting bias was identified in 109 of 211 (51.7\%). The median fragility index was 3.0 (IQR, 1.0-6.0). Multiplicity was detected in 175 trials (45.1\%), and only 35 (20.0\%) adjusted for multiple comparisons.
CONCLUSIONS AND RELEVANCE: In this systematic review, the size of contemporary surgical trials was small and the focus was on minor clinical events. Trial registration remained suboptimal and discrepancies with the published protocol and reporting bias were frequent. Few trials controlled for surgeon experience or assessed the quality of the intervention.},
	language = {eng},
	number = {6},
	journal = {JAMA network open},
	author = {Robinson, N. Bryce and Fremes, Stephen and Hameed, Irbaz and Rahouma, Mohamed and Weidenmann, Viola and Demetres, Michelle and Morsi, Mahmoud and Soletti, Giovanni and Di Franco, Antonino and Zenati, Marco A. and Raja, Shahzad G. and Moher, David and Bakaeen, Faisal and Chikwe, Joanna and Bhatt, Deepak L. and Kurlansky, Paul and Girardi, Leonard N. and Gaudino, Mario},
	month = jun,
	year = {2021},
	pmid = {34190996},
	pmcid = {PMC8246313},
	keywords = {Randomized Controlled Trials as Topic, Humans, Time Factors, General Surgery},
	pages = {e2114494},
	file = {Full Text:C\:\\Users\\Joe\\Zotero\\storage\\USS33JP4\\Robinson et al. - 2021 - Characteristics of Randomized Clinical Trials in S.pdf:application/pdf},
}

@book{european_medical_agency_guideline_2015,
	title = {Guideline on adjustment for baseline covariates in clinical trials},
	author = {{European Medical Agency}},
	year = {2015},
	note = {Report No.: EMA/CHMP/295050/2013},
}

@article{morris_using_2019,
	title = {Using simulation studies to evaluate statistical methods},
	volume = {38},
	issn = {1097-0258},
	doi = {10.1002/sim.8086},
	abstract = {Simulation studies are computer experiments that involve creating data by pseudo-random sampling. A key strength of simulation studies is the ability to understand the behavior of statistical methods because some "truth" (usually some parameter/s of interest) is known from the process of generating the data. This allows us to consider properties of methods, such as bias. While widely used, simulation studies are often poorly designed, analyzed, and reported. This tutorial outlines the rationale for using simulation studies and offers guidance for design, execution, analysis, reporting, and presentation. In particular, this tutorial provides a structured approach for planning and reporting simulation studies, which involves defining aims, data-generating mechanisms, estimands, methods, and performance measures ("ADEMP"); coherent terminology for simulation studies; guidance on coding simulation studies; a critical discussion of key performance measures and their estimation; guidance on structuring tabular and graphical presentation of results; and new graphical presentations. With a view to describing recent practice, we review 100 articles taken from Volume 34 of Statistics in Medicine, which included at least one simulation study and identify areas for improvement.},
	language = {eng},
	number = {11},
	journal = {Statistics in Medicine},
	author = {Morris, Tim P. and White, Ian R. and Crowther, Michael J.},
	month = may,
	year = {2019},
	pmid = {30652356},
	pmcid = {PMC6492164},
	keywords = {Guidelines as Topic, Research Design, Computer Simulation, Bias, Models, Statistical, Biostatistics, graphics for simulation, Monte Carlo, Monte Carlo Method, simulation design, simulation reporting, simulation studies},
	pages = {2074--2102},
	file = {Full Text:C\:\\Users\\Joe\\Zotero\\storage\\KV3Z5ZSH\\Morris et al. - 2019 - Using simulation studies to evaluate statistical m.pdf:application/pdf},
}

@article{schumi_through_2011,
	title = {Through the looking glass: understanding non-inferiority},
	volume = {12},
	issn = {1745-6215},
	shorttitle = {Through the looking glass},
	doi = {10.1186/1745-6215-12-106},
	abstract = {Non-inferiority trials test whether a new product is not unacceptably worse than a product already in use. This paper introduces concepts related to non-inferiority, and discusses the regulatory views of both the European Medicines Agency and the United States Food and Drug Administration.},
	language = {eng},
	journal = {Trials},
	author = {Schumi, Jennifer and Wittes, Janet T.},
	month = may,
	year = {2011},
	pmid = {21539749},
	pmcid = {PMC3113981},
	keywords = {Research Design, Humans, Treatment Outcome, Sample Size, Endpoint Determination, Animals, Clinical Trials as Topic, Device Approval, Drug Approval, Europe, Evidence-Based Medicine, Government Regulation, Risk Assessment, Therapeutic Equivalency, United States, United States Food and Drug Administration},
	pages = {106},
	file = {Full Text:C\:\\Users\\Joe\\Zotero\\storage\\A9M35T88\\Schumi and Wittes - 2011 - Through the looking glass understanding non-infer.pdf:application/pdf},
}

@article{nicholas_impact_2015,
	title = {The impact of covariate adjustment at randomization and analysis for binary outcomes: understanding differences between superiority and noninferiority trials},
	volume = {34},
	issn = {1097-0258},
	shorttitle = {The impact of covariate adjustment at randomization and analysis for binary outcomes},
	doi = {10.1002/sim.6447},
	abstract = {The question of when to adjust for important prognostic covariates often arises in the design of clinical trials, and there remain various opinions on whether to adjust during both randomization and analysis, at randomization alone, or at analysis alone. Furthermore, little is known about the impact of covariate adjustment in the context of noninferiority (NI) designs. The current simulation-based research explores this issue in the NI setting, as compared with the typical superiority setting, by assessing the differential impact on power, type I error, and bias in the treatment estimate as well as its standard error, in the context of logistic regression under both simple and covariate adjusted permuted block randomization algorithms. In both the superiority and NI settings, failure to adjust for covariates that influence outcome in the analysis phase, regardless of prior adjustment at randomization, results in treatment estimates that are biased toward zero, with standard errors that are deflated. However, as no treatment difference is approached under the null hypothesis in superiority and under the alternative in NI, this results in decreased power and nominal or conservative (deflated) type I error in the context of superiority but inflated power and type I error under NI. Results from the simulation study suggest that, regardless of the use of the covariate in randomization, it is appropriate to adjust for important prognostic covariates in analysis, as this yields nearly unbiased estimates of treatment as well as nominal type I error.},
	language = {eng},
	number = {11},
	journal = {Statistics in Medicine},
	author = {Nicholas, Katherine and Yeatts, Sharon D. and Zhao, Wenle and Ciolino, Jody and Borg, Keith and Durkalski, Valerie},
	month = may,
	year = {2015},
	pmid = {25641057},
	pmcid = {PMC4393769},
	keywords = {Randomized Controlled Trials as Topic, Research Design, Computer Simulation, Humans, Logistic Models, Prognosis, Algorithms, clinical trials, covariates, noninferiority, randomization},
	pages = {1834--1840},
	file = {Accepted Version:C\:\\Users\\Joe\\Zotero\\storage\\N3PEID2U\\Nicholas et al. - 2015 - The impact of covariate adjustment at randomizatio.pdf:application/pdf},
}

@article{kunzel_metalearners_2019,
	title = {Metalearners for estimating heterogeneous treatment effects using machine learning},
	volume = {116},
	issn = {1091-6490},
	doi = {10.1073/pnas.1804597116},
	abstract = {There is growing interest in estimating and analyzing heterogeneous treatment effects in experimental and observational studies. We describe a number of metaalgorithms that can take advantage of any supervised learning or regression method in machine learning and statistics to estimate the conditional average treatment effect (CATE) function. Metaalgorithms build on base algorithms-such as random forests (RFs), Bayesian additive regression trees (BARTs), or neural networks-to estimate the CATE, a function that the base algorithms are not designed to estimate directly. We introduce a metaalgorithm, the X-learner, that is provably efficient when the number of units in one treatment group is much larger than in the other and can exploit structural properties of the CATE function. For example, if the CATE function is linear and the response functions in treatment and control are Lipschitz-continuous, the X-learner can still achieve the parametric rate under regularity conditions. We then introduce versions of the X-learner that use RF and BART as base learners. In extensive simulation studies, the X-learner performs favorably, although none of the metalearners is uniformly the best. In two persuasion field experiments from political science, we demonstrate how our X-learner can be used to target treatment regimes and to shed light on underlying mechanisms. A software package is provided that implements our methods.},
	language = {eng},
	number = {10},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Künzel, Sören R. and Sekhon, Jasjeet S. and Bickel, Peter J. and Yu, Bin},
	month = mar,
	year = {2019},
	pmid = {30770453},
	pmcid = {PMC6410831},
	keywords = {randomized controlled trials, conditional average treatment effect, heterogeneous treatment effects, minimax optimality, observational studies},
	pages = {4156--4165},
	file = {Full Text:C\:\\Users\\Joe\\Zotero\\storage\\KATNZQ39\\Künzel et al. - 2019 - Metalearners for estimating heterogeneous treatmen.pdf:application/pdf},
}

@article{benkeser_improving_2021,
	title = {Improving precision and power in randomized trials for {COVID}-19 treatments using covariate adjustment, for binary, ordinal, and time-to-event outcomes},
	volume = {77},
	issn = {1541-0420},
	doi = {10.1111/biom.13377},
	abstract = {Time is of the essence in evaluating potential drugs and biologics for the treatment and prevention of COVID-19. There are currently 876 randomized clinical trials (phase 2 and 3) of treatments for COVID-19 registered on clinicaltrials.gov. Covariate adjustment is a statistical analysis method with potential to improve precision and reduce the required sample size for a substantial number of these trials. Though covariate adjustment is recommended by the U.S. Food and Drug Administration and the European Medicines Agency, it is underutilized, especially for the types of outcomes (binary, ordinal, and time-to-event) that are common in COVID-19 trials. To demonstrate the potential value added by covariate adjustment in this context, we simulated two-arm, randomized trials comparing a hypothetical COVID-19 treatment versus standard of care, where the primary outcome is binary, ordinal, or time-to-event. Our simulated distributions are derived from two sources: longitudinal data on over 500 patients hospitalized at Weill Cornell Medicine New York Presbyterian Hospital and a Centers for Disease Control and Prevention preliminary description of 2449 cases. In simulated trials with sample sizes ranging from 100 to 1000 participants, we found substantial precision gains from using covariate adjustment-equivalent to 4-18\% reductions in the required sample size to achieve a desired power. This was the case for a variety of estimands (targets of inference). From these simulations, we conclude that covariate adjustment is a low-risk, high-reward approach to streamlining COVID-19 treatment trials. We provide an R package and practical recommendations for implementation.},
	language = {eng},
	number = {4},
	journal = {Biometrics},
	author = {Benkeser, David and Díaz, Iván and Luedtke, Alex and Segal, Jodi and Scharfstein, Daniel and Rosenblum, Michael},
	month = dec,
	year = {2021},
	pmid = {32978962},
	pmcid = {PMC7537316},
	keywords = {Randomized Controlled Trials as Topic, Humans, Treatment Outcome, covariate adjustment, United States, COVID-19, COVID-19 Drug Treatment, Hospitalization, ordinal outcomes, randomized trial, SARS-CoV-2, survival analysis},
	pages = {1467--1481},
	file = {Full Text:C\:\\Users\\Joe\\Zotero\\storage\\CE9JE2G3\\Benkeser et al. - 2021 - Improving precision and power in randomized trials.pdf:application/pdf},
}

@article{morris_planning_2022,
	title = {Planning a method for covariate adjustment in individually randomised trials: a practical guide},
	volume = {23},
	issn = {1745-6215},
	shorttitle = {Planning a method for covariate adjustment in individually randomised trials},
	doi = {10.1186/s13063-022-06097-z},
	abstract = {BACKGROUND: It has long been advised to account for baseline covariates in the analysis of confirmatory randomised trials, with the main statistical justifications being that this increases power and, when a randomisation scheme balanced covariates, permits a valid estimate of experimental error. There are various methods available to account for covariates but it is not clear how to choose among them.
METHODS: Taking the perspective of writing a statistical analysis plan, we consider how to choose between the three most promising broad approaches: direct adjustment, standardisation and inverse-probability-of-treatment weighting.
RESULTS: The three approaches are similar in being asymptotically efficient, in losing efficiency with mis-specified covariate functions and in handling designed balance. If a marginal estimand is targeted (for example, a risk difference or survival difference), then direct adjustment should be avoided because it involves fitting non-standard models that are subject to convergence issues. Convergence is most likely with IPTW. Robust standard errors used by IPTW are anti-conservative at small sample sizes. All approaches can use similar methods to handle missing covariate data. With missing outcome data, each method has its own way to estimate a treatment effect in the all-randomised population. We illustrate some issues in a reanalysis of GetTested, a randomised trial designed to assess the effectiveness of an electonic sexually transmitted infection testing and results service.
CONCLUSIONS: No single approach is always best: the choice will depend on the trial context. We encourage trialists to consider all three methods more routinely.},
	language = {eng},
	number = {1},
	journal = {Trials},
	author = {Morris, Tim P. and Walker, A. Sarah and Williamson, Elizabeth J. and White, Ian R.},
	month = apr,
	year = {2022},
	pmid = {35436970},
	pmcid = {PMC9014627},
	keywords = {Research Design, Probability, Covariate adjustment, Humans, Sample Size, Clinical trials, Estimands, Inverse probability of treatment weighting, Missing data, Randomised controlled trials, Standardisation},
	pages = {328},
	file = {Full Text:C\:\\Users\\Joe\\Zotero\\storage\\LC2XLXQQ\\Morris et al. - 2022 - Planning a method for covariate adjustment in indi.pdf:application/pdf},
}

@article{schomaker_bootstrap_2018,
	title = {Bootstrap {Inference} {When} {Using} {Multiple} {Imputation}},
	volume = {37},
	issn = {0277-6715},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5986623/},
	doi = {10.1002/sim.7654},
	abstract = {Many modern estimators require bootstrapping to calculate confidence intervals because either no analytic standard error is available or the distribution of the parameter of interest is non-symmetric. It remains however unclear how to obtain valid bootstrap inference when dealing with multiple imputation to address missing data. We present four methods which are intuitively appealing, easy to implement, and combine bootstrap estimation with multiple imputation. We show that three of the four approaches yield valid inference, but that the performance of the methods varies with respect to the number of imputed data sets and the extent of missingness. Simulation studies reveal the behavior of our approaches in finite samples. A topical analysis from HIV treatment research, which determines the optimal timing of antiretroviral treatment initiation in young children, demonstrates the practical implications of the four methods in a sophisticated and realistic setting. This analysis suffers from missing data and uses the g-formula for inference, a method for which no standard errors are available.},
	number = {14},
	urldate = {2024-09-26},
	journal = {Statistics in medicine},
	author = {Schomaker, M. and Heumann, C.},
	month = jun,
	year = {2018},
	pmid = {29682776},
	pmcid = {PMC5986623},
	pages = {2252--2266},
	file = {PubMed Central Full Text PDF:C\:\\Users\\Joe\\Zotero\\storage\\MAYH6F23\\Schomaker and Heumann - 2018 - Bootstrap Inference When Using Multiple Imputation.pdf:application/pdf},
}

@article{rubin_estimating_1974,
	title = {Estimating causal effects of treatments in randomized and nonrandomized studies},
	volume = {66},
	issn = {1939-2176},
	doi = {10.1037/h0037350},
	abstract = {Presents a discussion of matching, randomization, random sampling, and other methods of controlling extraneous variation. The objective was to specify the benefits of randomization in estimating causal effects of treatments. It is concluded that randomization should be employed whenever possible but that the use of carefully controlled nonrandomized data to estimate causal effects is a reasonable and necessary procedure in many cases. (15 ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	number = {5},
	journal = {Journal of Educational Psychology},
	author = {Rubin, Donald B.},
	year = {1974},
	note = {Place: US
Publisher: American Psychological Association},
	keywords = {Experimental Design, Random Sampling},
	pages = {688--701},
	file = {Snapshot:C\:\\Users\\Joe\\Zotero\\storage\\5839ZJD8\\1975-06502-001.html:text/html},
}

@article{van_der_ploeg_modern_2014,
	title = {Modern modelling techniques are data hungry: a simulation study for predicting dichotomous endpoints},
	volume = {14},
	issn = {1471-2288},
	shorttitle = {Modern modelling techniques are data hungry},
	doi = {10.1186/1471-2288-14-137},
	abstract = {BACKGROUND: Modern modelling techniques may potentially provide more accurate predictions of binary outcomes than classical techniques. We aimed to study the predictive performance of different modelling techniques in relation to the effective sample size ("data hungriness").
METHODS: We performed simulation studies based on three clinical cohorts: 1282 patients with head and neck cancer (with 46.9\% 5 year survival), 1731 patients with traumatic brain injury (22.3\% 6 month mortality) and 3181 patients with minor head injury (7.6\% with CT scan abnormalities). We compared three relatively modern modelling techniques: support vector machines (SVM), neural nets (NN), and random forests (RF) and two classical techniques: logistic regression (LR) and classification and regression trees (CART). We created three large artificial databases with 20 fold, 10 fold and 6 fold replication of subjects, where we generated dichotomous outcomes according to different underlying models. We applied each modelling technique to increasingly larger development parts (100 repetitions). The area under the ROC-curve (AUC) indicated the performance of each model in the development part and in an independent validation part. Data hungriness was defined by plateauing of AUC and small optimism (difference between the mean apparent AUC and the mean validated AUC {\textless}0.01).
RESULTS: We found that a stable AUC was reached by LR at approximately 20 to 50 events per variable, followed by CART, SVM, NN and RF models. Optimism decreased with increasing sample sizes and the same ranking of techniques. The RF, SVM and NN models showed instability and a high optimism even with {\textgreater}200 events per variable.
CONCLUSIONS: Modern modelling techniques such as SVM, NN and RF may need over 10 times as many events per variable to achieve a stable AUC and a small optimism than classical modelling techniques such as LR. This implies that such modern techniques should only be used in medical prediction problems if very large data sets are available.},
	language = {eng},
	journal = {BMC medical research methodology},
	author = {{van der Ploeg}, Tjeerd and Austin, Peter C. and Steyerberg, Ewout W.},
	month = dec,
	year = {2014},
	pmid = {25532820},
	pmcid = {PMC4289553},
	keywords = {Humans, Treatment Outcome, Models, Statistical, Brain Injuries, Data Interpretation, Statistical, Databases, Factual, Endpoint Determination, Head and Neck Neoplasms, Head Injuries, Closed, ROC Curve, Support Vector Machine},
	pages = {137},
	file = {Full Text:C\:\\Users\\Joe\\Zotero\\storage\\6ALDXPSB\\van der Ploeg et al. - 2014 - Modern modelling techniques are data hungry a sim.pdf:application/pdf},
}

@book{senn_statistical_2021,
	address = {Hoboken, NJ},
	edition = {Third edition},
	series = {Statistics in practice},
	title = {Statistical issues in drug development},
	isbn = {978-1-119-23857-7 978-1-119-23861-4 978-1-119-23860-7},
	abstract = {"This will be the third edition of Statistical Issues in Drug Development, and will be fully revised and updated to include information on the latest industry standards and guidelines. Both the first (1997) and second (2007) editions were very well received and the book has become a standard. This book is unique in providing a thorough and critical discussion of the most important and controversial issues encountered by statisticians and their life scientist colleagues on both sides of the regulatory divide in drug development. The primary purpose of the book is to encourage and facilitate discussion between statisticians and their colleagues of the many complex statistical issues that arise in drug development. The book will be suitable as a course of self-instruction for statisticians who are new to the pharmaceutical industry, either because of recent graduation or change of career. It will also act as an authoritative reference for those working in drug development, and provide possible topics for discussion in journal forums"--},
	language = {eng},
	publisher = {Wiley Blackwell},
	author = {Senn, Stephen},
	year = {2021},
	doi = {10.1002/9781119238614},
	annote = {Includes bibliographical references and index},
}

@article{raad_evaluation_2020,
	title = {An evaluation of inverse probability weighting using the propensity score for baseline covariate adjustment in smaller population randomised controlled trials with a continuous outcome},
	volume = {20},
	issn = {1471-2288},
	doi = {10.1186/s12874-020-00947-7},
	abstract = {BACKGROUND: It is important to estimate the treatment effect of interest accurately and precisely within the analysis of randomised controlled trials. One way to increase precision in the estimate and thus improve the power for randomised trials with continuous outcomes is through adjustment for pre-specified prognostic baseline covariates. Typically covariate adjustment is conducted using regression analysis, however recently, Inverse Probability of Treatment Weighting (IPTW) using the propensity score has been proposed as an alternative method. For a continuous outcome it has been shown that the IPTW estimator has the same large sample statistical properties as that obtained via analysis of covariance. However the performance of IPTW has not been explored for smaller population trials ({\textless} 100 participants), where precise estimation of the treatment effect has potential for greater impact than in larger samples.
METHODS: In this paper we explore the performance of the baseline adjusted treatment effect estimated using IPTW in smaller population trial settings. To do so we present a simulation study including a number of different trial scenarios with sample sizes ranging from 40 to 200 and adjustment for up to 6 covariates. We also re-analyse a paediatric eczema trial that includes 60 children.
RESULTS: In the simulation study the performance of the IPTW variance estimator was sub-optimal with smaller sample sizes. The coverage of 95\% CI's was marginally below 95\% for sample sizes {\textless} 150 and ≥ 100. For sample sizes {\textless} 100 the coverage of 95\% CI's was always significantly below 95\% for all covariate settings. The minimum coverage obtained with IPTW was 89\% with n = 40. In comparison, regression adjustment always resulted in 95\% coverage. The analysis of the eczema trial confirmed discrepancies between the IPTW and regression estimators in a real life small population setting.
CONCLUSIONS: The IPTW variance estimator does not perform so well with small samples. Thus we caution against the use of IPTW in small sample settings when the sample size is less than 150 and particularly when sample size {\textless} 100.},
	language = {eng},
	number = {1},
	journal = {BMC medical research methodology},
	author = {Raad, Hanaya and Cornelius, Victoria and Chan, Susan and Williamson, Elizabeth and Cro, Suzie},
	month = mar,
	year = {2020},
	pmid = {32293286},
	pmcid = {PMC7092449},
	keywords = {Child, Computer Simulation, Covariate adjustment, Humans, Inverse probability weighting, Monte Carlo Method, Probability, Propensity score, Propensity Score, Randomised controlled trial, Randomized Controlled Trials as Topic, Regression Analysis, Sample Size, Small population, Small sample size},
	pages = {70},
	file = {Full Text:C\:\\Users\\Joe\\Zotero\\storage\\YYYNHSNG\\Raad et al. - 2020 - An evaluation of inverse probability weighting usi.pdf:application/pdf},
}

@article{riley_penalization_2021,
	title = {Penalization and shrinkage methods produced unreliable clinical prediction models especially when sample size was small},
	volume = {132},
	issn = {1878-5921},
	doi = {10.1016/j.jclinepi.2020.12.005},
	abstract = {OBJECTIVES: When developing a clinical prediction model, penalization techniques are recommended to address overfitting, as they shrink predictor effect estimates toward the null and reduce mean-square prediction error in new individuals. However, shrinkage and penalty terms ('tuning parameters') are estimated with uncertainty from the development data set. We examined the magnitude of this uncertainty and the subsequent impact on prediction model performance.
STUDY DESIGN AND SETTING: This study comprises applied examples and a simulation study of the following methods: uniform shrinkage (estimated via a closed-form solution or bootstrapping), ridge regression, the lasso, and elastic net.
RESULTS: In a particular model development data set, penalization methods can be unreliable because tuning parameters are estimated with large uncertainty. This is of most concern when development data sets have a small effective sample size and the model's Cox-Snell R2 is low. The problem can lead to considerable miscalibration of model predictions in new individuals.
CONCLUSION: Penalization methods are not a 'carte blanche'; they do not guarantee a reliable prediction model is developed. They are more unreliable when needed most (i.e., when overfitting may be large). We recommend they are best applied with large effective sample sizes, as identified from recent sample size calculations that aim to minimize the potential for model overfitting and precisely estimate key parameters.},
	language = {eng},
	journal = {Journal of Clinical Epidemiology},
	author = {Riley, Richard D. and Snell, Kym I. E. and Martin, Glen P. and Whittle, Rebecca and Archer, Lucinda and Sperrin, Matthew and Collins, Gary S.},
	month = apr,
	year = {2021},
	pmid = {33307188},
	pmcid = {PMC8026952},
	keywords = {Bias, Epidemiologic Research Design, Humans, Models, Statistical, Overfitting, Penalization, Prognosis, Reproducibility of Results, Risk prediction models, Sample size, Sample Size, Shrinkage, Uncertainty},
	pages = {88--96},
	file = {Full Text:C\:\\Users\\Joe\\Zotero\\storage\\SPAUTBLU\\Riley et al. - 2021 - Penalization and shrinkage methods produced unreli.pdf:application/pdf},
}

@article{steyerberg_clinical_2000,
	title = {Clinical trials in acute myocardial infarction: should we adjust for baseline characteristics?},
	volume = {139},
	issn = {0002-8703},
	shorttitle = {Clinical trials in acute myocardial infarction},
	doi = {10.1016/s0002-8703(00)90001-2},
	abstract = {BACKGROUND: Clinical trials concerning acute myocardial infarction often evaluate short-term death. Several baseline characteristics are predictors of death, most notably age. Adjustment for one or more predictors in a multivariable analysis may be considered to correct the estimate of the treatment effect for any imbalance that by chance may have occurred between the randomized groups. Moreover, adjustment results in a stratified estimate of the effect of treatment.
METHODS AND RESULTS: The effects of adjustment (correction for imbalance and stratification) were studied with logistic regression analysis in the Global Use of Strategies to Open Occluded Coronary Arteries (GUSTO)-I trial. The primary end point was 30-day death, which occurred in 6.3\% of 10,348 patients randomly assigned to tissue plasminogen activator and 7.3\% of 20,162 patients randomly assigned to streptokinase thrombolytic therapy. This is equivalent to an unadjusted odds ratio of 0.853. No significant imbalance had occurred for any of 17 baseline characteristics considered, including well-known demographic, presenting, and history characteristics. Adjusted for age, the odds ratio was 0.829, which is an 18\% increase in estimated effect on the logistic scale. When adjusted for 17 characteristics, the odds ratio was 0.820, an increase of 25\%. The increase in effect estimate was largely explained by the stratification effect and only partly by imbalance of predictors.
CONCLUSIONS: Adjustment for predictive baseline characteristics, even when largely balanced, may lead to clearly different estimates of the treatment effect on mortality rates. Adjustment for important predictors such as age is recommended in clinical trials studying patients with acute myocardial infarction.},
	language = {eng},
	number = {5},
	journal = {American Heart Journal},
	author = {Steyerberg, E. W. and Bossuyt, P. M. and Lee, K. L.},
	month = may,
	year = {2000},
	pmid = {10783203},
	keywords = {Aged, Bias, Female, Humans, Male, Middle Aged, Myocardial Infarction, Research Design, Streptokinase, Survival Rate, Thrombolytic Therapy, Tissue Plasminogen Activator, Treatment Outcome},
	pages = {745--751},
}

@article{pocock_subgroup_2002,
	title = {Subgroup analysis, covariate adjustment and baseline comparisons in clinical trial reporting: current practice and problems},
	volume = {21},
	issn = {0277-6715},
	shorttitle = {Subgroup analysis, covariate adjustment and baseline comparisons in clinical trial reporting},
	doi = {10.1002/sim.1296},
	abstract = {Clinical trial investigators often record a great deal of baseline data on each patient at randomization. When reporting the trial's findings such baseline data can be used for (i) subgroup analyses which explore whether there is evidence that the treatment difference depends on certain patient characteristics, (ii) covariate-adjusted analyses which aim to refine the analysis of the overall treatment difference by taking account of the fact that some baseline characteristics are related to outcome and may be unbalanced between treatment groups, and (iii) baseline comparisons which compare the baseline characteristics of patients in each treatment group for any possible (unlucky) differences. This paper examines how these issues are currently tackled in the medical journals, based on a recent survey of 50 trial reports in four major journals. The statistical ramifications are explored, major problems are highlighted and recommendations for future practice are proposed. Key issues include: the overuse and overinterpretation of subgroup analyses; the underuse of appropriate statistical tests for interaction; inconsistencies in the use of covariate-adjustment; the lack of clear guidelines on covariate selection; the overuse of baseline comparisons in some studies; the misuses of significance tests for baseline comparability, and the need for trials to have a predefined statistical analysis plan for all these uses of baseline data.},
	language = {eng},
	number = {19},
	journal = {Statistics in Medicine},
	author = {Pocock, Stuart J. and Assmann, Susan E. and Enos, Laura E. and Kasten, Linda E.},
	month = oct,
	year = {2002},
	pmid = {12325108},
	keywords = {Antihypertensive Agents, Data Interpretation, Statistical, Female, Humans, Male, Myocardial Infarction, Periodicals as Topic, Publications, Randomized Controlled Trials as Topic, Social Support},
	pages = {2917--2930},
}

@article{lingsma_covariate_2010-1,
	title = {Covariate adjustment increases statistical power in randomized controlled trials},
	volume = {63},
	issn = {1878-5921},
	doi = {10.1016/j.jclinepi.2010.05.003},
	language = {eng},
	number = {12},
	journal = {Journal of Clinical Epidemiology},
	author = {Lingsma, Hester and Roozenbeek, Bob and Steyerberg, Ewout and {IMPACT investigators}},
	month = dec,
	year = {2010},
	pmid = {20800991},
	keywords = {Analysis of Variance, Data Interpretation, Statistical, Humans, Periodicals as Topic, Randomized Controlled Trials as Topic, Treatment Outcome},
	pages = {1391; author reply 1392--1393},
}
